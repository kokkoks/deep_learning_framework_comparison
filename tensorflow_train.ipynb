{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffeba22-068c-4ef5-9da7-b17df38d9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Custom libraries\n",
    "from tensorflow_generator import TensorflowDataGenerator, TensorflowDataGenerator_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df31839-a3a4-429f-9274-715c84d6c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set the CNN parameters\n",
    "num_epoch = 20\n",
    "batch_size = 6\n",
    "framework = \"tensorflow\"\n",
    "train_type = \"manual\"\n",
    "im_size = 224\n",
    "num_im = 1000\n",
    "predict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ca9cd-a0ac-4b3b-82c4-810f9cbb6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "root_path = \"./\"\n",
    "data_dir = os.path.join(root_path, \"dogs-vs-cats/train\")\n",
    "log_dir = os.path.join(\n",
    "    root_path,\n",
    "    \"logs\",\n",
    "    framework,\n",
    "    model_type,\n",
    "    \"logs\",\n",
    "    train_type + \"_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "if predict:\n",
    "    ckpt_name = f\"{train_type}_20200816-184346\"\n",
    "    ckpt_dir = os.path.join(\n",
    "        root_path, \"logs\", framework, model_type, \"ckpts\", ckpt_name\n",
    "    )\n",
    "else:\n",
    "    ckpt_dir = os.path.join(\n",
    "        root_path,\n",
    "        \"logs\",\n",
    "        framework,\n",
    "        model_type,\n",
    "        \"ckpts\",\n",
    "        train_type + \"_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74871191-aaf2-4b7f-9acd-c9d1743f12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input pipeline\n",
    "train_dir = os.path.join(root_path, \"dogs-vs-cats/train\")\n",
    "test_dir = os.path.join(root_path, \"dogs-vs-cats/test\")\n",
    "\n",
    "train_gen = TensorflowDataGenerator(\n",
    "    train_dir, batch_size, im_size=im_size, num_im=num_im, shuffle=True\n",
    ")\n",
    "val_imgs = train_gen.load_val()\n",
    "test_gen = TensorflowDataGenerator_Test(test_dir, batch_size, im_size=im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1ed58-6caa-4655-b43c-2b2eea3997e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "model = Sequential(\n",
    "    [\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            3,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            input_shape=(im_size, im_size, 3),\n",
    "        ),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "# Compile model\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b46d35-dc60-4fe0-bdf0-c5a28ef95359",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "train_acc = tf.keras.metrics.Mean()\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "val_acc = tf.keras.metrics.Mean()\n",
    "val_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    acc_value = tf.math.equal(\n",
    "        y, tf.math.round(tf.keras.activations.sigmoid(logits))\n",
    "    )\n",
    "    train_acc.update_state(acc_value)\n",
    "    train_loss.update_state(loss_value)\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_logits)\n",
    "    acc_value = tf.math.equal(\n",
    "        y, tf.math.round(tf.keras.activations.sigmoid(val_logits))\n",
    "    )\n",
    "    val_acc.update_state(acc_value)\n",
    "    val_loss.update_state(loss_value)\n",
    "    return loss_value, acc_value\n",
    "\n",
    "# Setup tensorboard\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "best_val_acc = 0  # for model check pointing\n",
    "# # Epoch loop\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    start_time = timer()\n",
    "    # Training loop\n",
    "    for inputs, targets in train_gen:\n",
    "        train_step(inputs, targets)\n",
    "\n",
    "    # Validation loop\n",
    "    for batch_idx in range(0, len(val_imgs[1]), batch_size):\n",
    "        inputs = val_imgs[0][batch_idx : batch_idx + batch_size, ...]\n",
    "        targets = val_imgs[1][batch_idx : batch_idx + batch_size]\n",
    "        test_step(inputs, targets)\n",
    "\n",
    "    # Log metrics to tensorboard\n",
    "    end_time = timer()\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar(\"Loss/train\", train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Loss/validation\", val_loss.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Accuracy/train\", train_acc.result(), step=epoch)\n",
    "        tf.summary.scalar(\"Accuracy/validation\", val_acc.result(), step=epoch)\n",
    "        tf.summary.scalar(\"epoch_time\", end_time - start_time, step=epoch)\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    print(\n",
    "        f\"Epoch: {epoch} \\tTraining Loss: {train_loss.result()} \\tValidation Loss: {val_loss.result()} \\tTraining Accuracy: {train_acc.result()} \\tValidation Accuracy: {val_acc.result()} \\tTime taken: {end_time - start_time}\"\n",
    "    )\n",
    "\n",
    "    # checkpoint if improved\n",
    "    if val_acc.result() > best_val_acc:\n",
    "        model.save_weights(ckpt_dir)\n",
    "        best_val_acc = val_acc.result()\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    val_acc.reset_states()\n",
    "    val_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206b9e6-de59-48d5-afee-864d4822ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_type == \"auto\":\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "    # Setup callbacks\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=ckpt_dir,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_imgs,\n",
    "        validation_steps=len(val_imgs[0]) // batch_size,\n",
    "        epochs=num_epoch,\n",
    "        callbacks=[tensorboard_callback, model_checkpoint_callback],\n",
    "        use_multiprocessing=True,\n",
    "        workers=8,\n",
    "    )\n",
    "\n",
    "elif train_type == \"manual\":\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "    train_acc = tf.keras.metrics.Mean()\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    val_acc = tf.keras.metrics.Mean()\n",
    "    val_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            loss_value = loss_fn(y, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        acc_value = tf.math.equal(\n",
    "            y, tf.math.round(tf.keras.activations.sigmoid(logits))\n",
    "        )\n",
    "        train_acc.update_state(acc_value)\n",
    "        train_loss.update_state(loss_value)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(x, y):\n",
    "        val_logits = model(x, training=False)\n",
    "        loss_value = loss_fn(y, val_logits)\n",
    "        acc_value = tf.math.equal(\n",
    "            y, tf.math.round(tf.keras.activations.sigmoid(val_logits))\n",
    "        )\n",
    "        val_acc.update_state(acc_value)\n",
    "        val_loss.update_state(loss_value)\n",
    "        return loss_value, acc_value\n",
    "\n",
    "    # Setup tensorboard\n",
    "    file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "    best_val_acc = 0  # for model check pointing\n",
    "    # Epoch loop\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        start_time = timer()\n",
    "        # Training loop\n",
    "        for inputs, targets in train_gen:\n",
    "            train_step(inputs, targets)\n",
    "\n",
    "        # Validation loop\n",
    "        for batch_idx in range(0, len(val_imgs[1]), batch_size):\n",
    "            inputs = val_imgs[0][batch_idx : batch_idx + batch_size, ...]\n",
    "            targets = val_imgs[1][batch_idx : batch_idx + batch_size]\n",
    "            test_step(inputs, targets)\n",
    "\n",
    "        # Log metrics to tensorboard\n",
    "        end_time = timer()\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar(\"Loss/train\", train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar(\"Loss/validation\", val_loss.result(), step=epoch)\n",
    "            tf.summary.scalar(\"Accuracy/train\", train_acc.result(), step=epoch)\n",
    "            tf.summary.scalar(\"Accuracy/validation\", val_acc.result(), step=epoch)\n",
    "            tf.summary.scalar(\"epoch_time\", end_time - start_time, step=epoch)\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        print(\n",
    "            f\"Epoch: {epoch} \\tTraining Loss: {train_loss.result()} \\tValidation Loss: {val_loss.result()} \\tTraining Accuracy: {train_acc.result()} \\tValidation Accuracy: {val_acc.result()} \\tTime taken: {end_time - start_time}\"\n",
    "        )\n",
    "\n",
    "        # checkpoint if improved\n",
    "        if val_acc.result() > best_val_acc:\n",
    "            model.save_weights(ckpt_dir)\n",
    "            best_val_acc = val_acc.result()\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_acc.reset_states()\n",
    "        train_loss.reset_states()\n",
    "        val_acc.reset_states()\n",
    "        val_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575111a-cf78-4df8-88b3-cdbd4e1aaf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ckpt_dir)\n",
    "for inputs in test_gen:\n",
    "    outputs = model(inputs, training=False)\n",
    "    break\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142dd5d-081e-40d8-be4b-c49dcb428624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
